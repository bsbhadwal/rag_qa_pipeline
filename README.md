# RAG-based Code Q&A System

This project is a functional prototype of a system that can answer developer questions about any open-source GitHub code repository. It uses a sophisticated Retrieval-Augmented Generation (RAG) approach to provide accurate, context-aware answers through a simple web interface.

The primary objective is to build an AI-powered Q&A system over the contents of a GitHub repository, helping developers with onboarding, debugging, and general code comprehension tasks.

## Key Features

* **Code Ingestion**: Clones any public GitHub repository for analysis.
* **Semantic Code Chunking**: Parses code files into meaningful semantic units like functions and classes using `LlamaIndex's CodeSplitter`.
* **Hybrid Chunk Enrichment**: Each code chunk is enriched with a natural-language summary generated by an LLM, bridging the gap between code and natural language queries.
* **Vector-Based Retrieval**: Embeds the enriched chunks and stores them in a local ChromaDB vector store for efficient retrieval.
* **Retrieval-Augmented Generation (RAG)**: Uses retrieved code chunks to provide context to a powerful Large Language Model (LLM) to generate accurate answers.
* **Web Interface**: Exposes a minimal, user-friendly chat interface for interacting with the system.
* **Modular and Testable**: Features clean code structure, a modular design, and unit tests for major components.

## Tech Stack

The system is built with a selection of modern, efficient libraries and tools:

| Purpose         | Library / Tool                           |
| ----------------------- | ------------------------------------------------------------------ |
| RAG Framework      | LlamaIndex                   |
| Code Chunking      | `CodeSplitter` (LlamaIndex)          |
| Embedding Model     | `sentence-transformers/all-MiniLM-L6-v2` (default) |
| Vector Store      | ChromaDB (local, persistent)          |
| Language Model     | Google Gemini Flash (via API)         |
| Web Framework      | Flask                       |
| Configuration      | `python-dotenv`                |

## Prerequisites

Before you begin, ensure you have the following installed:
* Python 3.8+
* Git

## Setup and Installation

Follow these steps to set up the project locally.

**1. Clone the Repository**

```bash
git clone <your-repository-url>
cd <repository-directory>
```

**2. Create a Virtual Environment**

It is highly recommended to use a virtual environment to manage dependencies.

```bash
# For Unix/macOS
python3 -m venv venv
source venv/bin/activate

# For Windows
python -m venv venv
.\venv\Scripts\activate
```

**3. Install Dependencies**

Install the required Python libraries.

```bash
pip install -r requirements.txt
```

**4. Set Up Environment Variables**

The application uses a Google Gemini model for LLM tasks and requires an API key.

* Create a file named `.env` in the root directory of the project.
* Add your Google API key to the file: (MADE AVAILABLE IN EMAIL)

```env
GOOGLE_API_KEY="your_google_api_key_here"
```

## Running the Application

There are two main ways to run the system: the full pipeline via the command line for indexing, and the web application for querying.

### Step 1: Build the Index (One-Time Setup)

Before you can ask questions, you must first process the target repository and build the vector index. The default repository is `psf/requests`.

Run the main pipeline script from the root directory:

```bash
python3 pipeline.py
```

This script will:

1. Clone the repository specified in `config.py` into the `pipeline_outputs/cloned_repo` directory.
2. Extract code chunks from the repository files.
3. Generate LLM-powered summaries for each chunk.
4. Embed and store these hybrid chunks in a persistent ChromaDB vector store located at `pipeline_outputs/chroma_db_persistent`.
5. Run a set of default questions against the newly built index if any are defined in `config.py`.

**Note:** This process can take some time, especially the first time you run it, as it involves LLM calls to summarize every code chunk. Subsequent runs will use the cached index.

### Step 2: Run the Web Application

Once the index is built, you can start the interactive web UI.

```bash
python3 app.py
```


This will start a Flask development server, typically at `http://127.0.0.1:5001`. On the first request, the application will load the models and the pre-built index from disk.

Navigate to this URL in your web browser. You can now ask questions about the codebase.

## How to Use the Web UI

1. Open your web browser to the application's URL.
2. In the text area, type a question about the codebase. Examples include:
  * `What does the <function_name> do?`
  * `Explain how a <feature> works?`
  * `What is the return type of <function>?`
3. Click the "Get Answer" button.
4. The system will display the generated answer along with the source file references that were used as context.

## Project Structure

```text
.
├── web
|  └── app.py               # Flask web application entry point
   ├── templates/
      └── index.html        # HTML template for the web UI
├── config.py               # Centralized configuration for models, paths, etc.
├── constants.py            # Holds prompts, log messages, and other constants
├── DESIGN.md               # Detailed design document explaining architectural choices
├── pipeline.py             # Core RAG pipeline logic for indexing and querying
├── test_pipeline.py        # Unit tests for the core pipeline
├── .env                    # (AVAILABLE IN EMAIL) For storing API keys
└── README.md               # This file
└── DESIGN.md               # Design decisions and architecture
```

## Configuration

The project is highly configurable through the `config.py` file. You can easily modify:

* The target GitHub repository (`DEFAULT_REPO_URL`).
* LLM and embedding model names (`LLM_MODEL_NAME`, `EMBEDDING_MODEL_NAME`).
* Code chunking parameters (`CODE_SPLITTER_CHUNK_LINES`, etc.).
* Vector store collection name and directory paths.

## Testing

Unit tests are provided for the core components of the pipeline. To run the tests, execute the `test_pipeline.py` file:

```bash
python test_pipeline.py
```