# RAG-based Code Q&A System

This project is a functional prototype of a system that can answer developer questions about any open-source GitHub code repository. It uses a sophisticated Retrieval-Augmented Generation (RAG) approach to provide accurate, context-aware answers through a simple web interface.

The primary objective is to build an AI-powered Q&A system over the contents of a GitHub repository, helping developers with onboarding, debugging, and general code comprehension tasks.

## Key Features

* **Code Ingestion**: Clones any public GitHub repository for analysis.
* **Semantic Code Chunking**: Parses code files into meaningful semantic units like functions and classes using `LlamaIndex's CodeSplitter`.
* **Hybrid Chunk Enrichment**: Each code chunk is enriched with a natural-language summary generated by an LLM, bridging the gap between code and natural language queries.
* **Vector-Based Retrieval**: Embeds the enriched chunks and stores them in a local ChromaDB vector store for efficient retrieval.
* **Retrieval-Augmented Generation (RAG)**: Uses retrieved code chunks to provide context to a powerful Large Language Model (LLM) to generate accurate answers.
* **Web Interface**: Exposes a minimal, user-friendly chat interface for interacting with the system.
* **Modular and Testable**: Features clean code structure, a modular design, and unit tests for major components.

## Tech Stack

The system is built with a selection of modern, efficient libraries and tools:

| Purpose         | Library / Tool                           |
| ----------------------- | ------------------------------------------------------------------ |
| RAG Framework      | LlamaIndex                   |
| Code Chunking      | `CodeSplitter` (LlamaIndex)          |
| Embedding Model     | `sentence-transformers/all-MiniLM-L6-v2` (default) |
| Vector Store      | ChromaDB (local, persistent)          |
| Language Model     | Google Gemini Flash (via API)         |
| Web Framework      | Flask                       |
| Configuration      | `python-dotenv`                |

## Prerequisites

Before you begin, ensure you have the following installed:

* Python 3.8+
* Git

## Setup and Installation

Follow these steps to set up the project locally.

### 1. Clone the Repository

```bash
git clone [repository-url](https://github.com/bsbhadwal/rag_qa_pipeline.git)
cd <repository-directory>
```

### 2. Create a Virtual Environment

It is highly recommended to use a virtual environment to manage dependencies.

```bash
# For Unix/macOS
python3 -m venv ./.venv
source ./.venv/bin/activate

# For Windows
python -m venv ./.venv
.\.venv\Scripts\activate
```

### 3. Install Dependencies

Install the required Python libraries. This will be time consuming as it installs NVIDIA/CUDA specific libs too.

```bash
pip install -r requirements.txt
```

### 4. Set Up Environment Variables

The application uses a Google Gemini model for LLM tasks and requires an API key.

* Create a file named `.env` in the root directory of the project.
* Copy the contents from the env.txt file to this file: ( FILE MADE AVAILABLE IN EMAIL)

```env
# Temporary personal API key for prototype use (bsbhadwal) — rate limited to 15 requests/minute and 1500 requests/day.
# This key will be deactivated in the future. You can generate your own at: https://aistudio.google.com/
GOOGLE_API_KEY=<YOUR-KEY>
LLM_MODEL_NAME=<MODEL-NAME>
#OPTIONAL - keeps chroma db from sending so-called "anonymous" telemetry :P
ANONYMIZED_TELEMETRY="False"
```

## Running the Application

There are two main ways to run the system: the full pipeline via the command line for indexing, and the web application for querying.

### Step 1: Build the Index (One-Time Setup)

Before you can ask questions, you must first process the target repository and build the vector index. The default repository is `psf/requests`. Defined in config.py

Run the main pipeline script from the root directory:

```bash
python3 pipeline.py
```

This script will:

1. Clone the repository specified in `config.py` defaults to the `pipeline_outputs/cloned_repo` directory.
2. Extract code chunks from the repository files. Caches to default `pipeline_outputs/pipeline_cached_data`
3. Generate LLM-powered summaries for each chunk. `pipeline_outputs/pipeline_cached_data`
4. Embed and store these hybrid chunks in a persistent ChromaDB vector store located at `pipeline_outputs/chroma_db_persistent`.
5. Runs a set of default questions against the newly built index if any are defined in `config.py`. By default we have a set of over 20 Questions sorted from easy to hard that establishes what questions the RAG system can handle. The system also provides **references** for the answer along with their **relevance score**.

**Note:** This process can take some time, especially the first time you run it, as it involves LLM calls to summarize every code chunk (Plus delay to stay within free tier).  

( All tasks outputs are cached on file system which adds redundancy / saves processing time. Subsequent runs will use the cached index.)

**Command Line Only** There is no need to run the web UI to simply answer questions, you can do it via command line. Simply add your preferred repository and set of questions to the `config.py`.

### Step 2: Run the Web Application

Once the index is built, you can start the interactive web UI.

```bash
python3 web/app.py
```

This will start a Flask development server, at `http://localhost:5001`. On the first request, the application will load the models and the pre-built index from disk. 5001 was chosen so it doesnt interfere with any existing flask applications that run at port :5000

Navigate to this URL in your web browser. You can now ask questions about the codebase.

## How to Use the Web UI

1. Open your web browser to the application's URL.
2. In the text area, type a question about the codebase. Examples include:

  * `What does the <function_name> do?`
  * `Explain how a <feature> works?`
  * `What is the return type of <function>?`
(See config.py for a comprehensive list of questions)

3. Click the "Get Answer" button.
4. The system will display the generated answer along with the source file references and their relevance score that were used as context.

## Project Structure

```text
.
├── web
|  └── app.py               # Flask web application entry point
   ├── templates/
      └── index.html        # HTML template for the web UI
├── config.py               # Centralized configuration for models, paths, etc.
├── constants.py            # Holds prompts, log messages, and other constants
├── DESIGN.md               # Detailed design document explaining architectural choices
├── pipeline.py             # Core RAG pipeline logic for indexing and querying
├── test_pipeline.py        # Unit tests for the core pipeline
├── .env                    # (AVAILABLE IN EMAIL) For storing API keys
└── README.md               # This file
└── DESIGN.md               # Design decisions and architecture
```

## Configuration

The project is highly configurable through the `config.py` file. You can easily modify:

* The target GitHub repository (`DEFAULT_REPO_URL`).
* LLM and embedding model names (`LLM_MODEL_NAME`, `EMBEDDING_MODEL_NAME`).
* Code chunking parameters (`CODE_SPLITTER_CHUNK_LINES`, etc.).
* Vector store collection name and directory paths.

## To work with a New Github Repository

* Delete existing Output Directory (defined in confiig.py). Default: BASE_OUTPUT_DIR_NAME = "pipeline_outputs"
* Change the Github repository URL in the config.py. DEFAULT_REPO_URL=new_github_public_repo_url
* Run pipeline.py. This will recreate the output directory/cache with the new Github Repository.

## Testing

Unit tests are provided for the core components of the pipeline. To run the tests, execute the `test_pipeline.py` file:

```bash
python tests/test_pipeline.py
```
